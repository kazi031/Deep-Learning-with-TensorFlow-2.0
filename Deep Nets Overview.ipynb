{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7a07c4",
   "metadata": {},
   "source": [
    "## Input Layer -> Hidden Layers -> Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d370b4b",
   "metadata": {},
   "source": [
    "Parameters Vs Hyperparameters.\n",
    "Parameters are weights(w) and biases. Whereas Hyperparameters are Width, Depth and Learning rate.\n",
    "The value of parameters can be found by optimization. Whereas hyperparameters are preset by us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0eb0cf",
   "metadata": {},
   "source": [
    "### Stacking Layers is the process of placing one Layer after the other in a meaningful way. We cannot stack Layers when we only have linear relationships. \n",
    "### Withour non-linearity stacking Layers one after the another is meaning-less. Two consecutive linear transformations are equivalent to a single one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0df0b3",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "\n",
    "Activation functions transform inputs into outputs of different kind. \n",
    "\n",
    "Sigmoid (Logistic function) -> \n",
    "TanH (Hyperbolic tangent) ->\n",
    "ReLu (Rectified Linear unit) ->\n",
    "softmax ->\n",
    "\n",
    "* Activation functions are also called transer functions.\n",
    "\n",
    "* Softmax Function => Range (0,1) They always sum up to 1. Like probabilities.\n",
    "The softmax takes as argument the whole vector instead of individual elements. The softmax transformation transforms a bunch of arbitrarily large or small numbers into a valid probability distribution. \n",
    "\n",
    "* Softmax is often used as the activation of the output Layer in classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960ae18",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Forward Propagation : Forward Propagation is the process of pushing inputs through the net. At the end of each \"epoch\"\n",
    "  the obtained outputs are compared to the targets to form the errors. \n",
    " \n",
    "* Backpropagation:  Then, we backpropagate through partial derivatives and change each parameter so error at the next epoch are \n",
    "  minimized.\n",
    " \n",
    "So essentially through backpropagation the algorith identifies which weights lead to which errors.\n",
    "\n",
    "### A big problem arrises when we take in account the activation function. Bacause they introduce additional complexity to this process. That's why backpropagation is one the biggest challenges for the speed of an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40321378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2]",
   "language": "python",
   "name": "conda-env-py3-TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
